# A Web Crawler With asyncio Coroutines

这是[开源程序架构](http://aosabook.org/en/index.html)系列的第四本[500 Lines or Less](https://github.com/aosabook/500lines/blob/master/README.md)的早期章节。
如果你发现任何问题，可以在我们的[Github追踪器](https://github.com/aosabook/500lines/issues)上反馈。
请关注[AOSA blog](http://aosabook.org/blog/)或新的章节和最后的出版计划，新闻公告[推特](https://twitter.com/aosabook), 获取关于本书的最新消息。
***

A. Jesse Jiryu Davis is a staff engineer at MongoDB in New York. He wrote Motor, the async MongoDB Python driver,
 and he is the lead developer of the MongoDB C Driver and a member of the PyMongo team. He contributes to asyncio and Tornado. 
 He writes at http://emptysqua.re.
A. Jesse Jiryu Davis在纽约为MongoDB工作。他编写了Motor，异步MongoDB Python驱动器，他也是MongoDB C驱动器的首席开发者，
同时他也是PyMango组织的成员之一。他对asyncio和Tornado同样有着杰出贡献。他的博客是 http://emptysqua.re .

Guido van Rossum is the creator of Python, one of the major programming languages on and off the web. 
The Python community refers to him as the BDFL (Benevolent Dictator For Life), a title straight from a Monty Python skit. 
Guido's home on the web is http://www.python.org/~guido/.
Guido van Rossum，Python之父，Python是目前主要的编程语言之一，无论线上线下。
他在社区里一直是一位仁慈的独裁者，一个来自Monty Python短剧的标题。Guido网上的家是http://www.python.org/~guido/ .

## 介绍

Classical computer science emphasizes efficient algorithms that complete computations as quickly as possible. 
But many networked programs spend their time not computing, but holding open many connections that are slow, 
or have infrequent events. These programs present a very different challenge: to wait for a huge number of network events efficiently.
A contemporary approach to this problem is asynchronous I/O, or "async".
经典计算机科学看重高效的算法以便能尽快完成计算。但是许多网络程序消耗时间不在计算上，而是维持打开许多缓慢的连接，或者有其他不频繁的事件。
这些程序表达了一个不同的挑战：如何高效的监听大量网络事件。一个现代的方法是异步I/O.

This chapter presents a simple web crawler. The crawler is an archetypal async application because it waits for many responses,
but does little computation. The more pages it can fetch at once, the sooner it completes. 
If it devotes a thread to each in-flight request, then as the number of concurrent requests rises it will run out of memory or 
other thread-related resource before it runs out of sockets. It avoids the need for threads by using asynchronous I/O.
这一章节实现了一个简单的网络爬虫。爬虫是一个异步应用因为它需要等待许多响应，而极少有计算。它每次可以抓取的页面越多，它完成的时间越久。
如果它为每一个运行的请求分发一个进程，那么随着并发请求数量的增加，它会耗尽内存，或者在它之前的线程相关的程序用光套接字。
它通过使用异步IO来避免大量进程需求。

We present the example in three stages. First, we show an async event loop and sketch a crawler that uses the event loop 
with callbacks: it is very efficient, but extending it to more complex problems would lead to unmanageable spaghetti code. 
Second, therefore, we show that Python coroutines are both efficient and extensible. 
We implement simple coroutines in Python using generator functions. In the third stage, 
we use the full-featured coroutines from Python's standard "asyncio" library1, and coordinate them using an async queue.
我们通过三步来实现这个例子。首先，我们展示一个异步事件循环并且梗概一个通过回掉使用这个事件循环。它非常高效，但是扩展它去适应更复杂的问题时会
导致难以处理的意大利面条式代码。因而接下来我们展示既高效又易扩展的Python协同程序。我们在Python中使用生成器函数来实现简单的协调程序。
最后，我们使用来自Python标准“asyncio”库中的全功能的协程程序，然后使用异步序列来整合他们。

## 任务
A web crawler finds and downloads all pages on a website, perhaps to archive or index them. Beginning with a root URL, 
it fetches each page, parses it for links to pages it has not seen, and adds the new links to a queue. 
When it fetches a page with no unseen links and the queue is empty, it stops.
一个网络爬虫寻找并下载一个网站上的所有页面，或者存储，或者建立索引。由一个根地址开始，取得每一个页面，解析它去寻找指向从未访问过的页面的链接，
把新的链接加入对了。当他解析到一个没有可见链接的页面并且队列是空的，它便停下来。

We can hasten this process by downloading many pages concurrently. As the crawler finds new links, 
it launches simultaneous fetch operations for the new pages on separate sockets. It parses responses as they arrive, adding new links to the queue. 
There may come some point of diminishing returns where too much concurrency degrades performance, so we cap the number of concurrent requests, 
and leave the remaining links in the queue until some in-flight requests complete.
我们可以通过同时下载许多页面来加快这个过程。随着爬虫发现新的链接，它为每一个单独套接字上绑定的新页面启动同时抓取操作。当他们抵达时，他开始解析响应，往队列里添加新的链接。
大量的并发请求可能导致一些性能降低，因而我们限制同一时间内请求的数量，把其他的链接加入队列直到一些运行中的请求完成。

## 传统方法的实现
How do we make the crawler concurrent? Traditionally we would create a thread pool. 
Each thread would be in charge of downloading one page at a time over a socket. For example, to download a page from xkcd.com:
我们该如何让爬虫并发呢？传统方法是建立一个进程池。每个进程每次将负责通过一个套接字下载一个页面。比如，下载“xkcd.com”的一个页面。

```
def fetch(url):
    sock = socket.socket()
    sock.connect(('xkcd.com', 80))
    request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
    sock.send(request.encode('ascii'))
    response = b''
    chunk = sock.recv(4096)
    while chunk:
        response += chunk
        chunk = sock.recv(4096)

    # Page is now downloaded.
    links = parse_links(response)
    q.add(links)
```

By default, socket operations are blocking: when the thread calls a method like connect or recv, it pauses until the operation completes.
2 Consequently to download many pages at once, we need many threads. A sophisticated application amortizes the cost of thread-creation 
by keeping idle threads in a thread pool, then checking them out to reuse them for subsequent tasks; it does the same with sockets in a connection pool.
默认情况下，套接字操作是分块的：当进程调用一个方法，比如说连```connect```或者```recv```，它会暂停直到操作完成。

And yet, threads are expensive, and operating systems enforce a variety of hard caps on the number of threads a process, user, or machine may have. 
On Jesse's system, a Python thread costs around 50k of memory, and starting tens of thousands of threads causes failures. 
If we scale up to tens of thousands of simultaneous operations on concurrent sockets, we run out of threads before we run out of sockets. 
Per-thread overhead or system limits on threads are the bottleneck.

In his influential article "The C10K problem"3, Dan Kegel outlines the limitations of multithreading for I/O concurrency. He begins,

> It's time for web servers to handle ten thousand clients simultaneously, don't you think? After all, the web is a big place now.
> 是时候让web服务器同时处理数万客户端请求了，不是吗？毕竟，web那么大。

Kegel coined the term "C10K" in 1999. Ten thousand connections sounds dainty now, but the problem has changed only in size, 
not in kind. Back then, using a thread per connection for C10K was impractical. Now the cap is orders of magnitude higher. 
Indeed, our toy web crawler would work just fine with threads. Yet for very large scale applications,
with hundreds of thousands of connections, the cap remains: there is a limit beyond which most systems can still create sockets, 
but have run out of threads. How can we overcome this?

## 异步

## 回调

## 协程

## Python生成器如何工作

## 使用生成器实现协程

So a generator can pause, and it can be resumed with a value, and it has a return value. Sounds like a good primitive upon which to build an async programming model, 
without spaghetti callbacks! We want to build a "coroutine": a routine that is cooperatively scheduled with other routines in the program. 
Our coroutines will be a simplified version of those in Python's standard "asyncio" library. As in asyncio, we will use generators, futures, 
and the "yield from" statement.

First we need a way to represent some future result that a coroutine is waiting for. A stripped-down version:

```
class Future:
    def __init__(self):
        self.result = None
        self._callbacks = []

    def add_done_callback(self, fn):
        self._callbacks.append(fn)

    def set_result(self, result):
        self.result = result
        for fn in self._callbacks:
            fn(self)
```

A future is initially "pending". It is "resolved" by a call to ```set_result```.9

Let us adapt our fetcher to use futures and coroutines. Review how we wrote ```fetch``` with a callback:

```
class Fetcher:
    def fetch(self):
        self.sock = socket.socket()
        self.sock.setblocking(False)
        try:
            self.sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass
        selector.register(self.sock.fileno(),
                          EVENT_WRITE,
                          self.connected)

    def connected(self, key, mask):
        print('connected!')
        # And so on....
```

The fetch method begins connecting a socket, then registers the callback, connected, to be executed when the socket is ready. 
Now we can combine these two steps into one coroutine:

```
    def fetch(self):
        sock = socket.socket()
        sock.setblocking(False)
        try:
            sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass

        f = Future()

        def on_connected():
            f.set_result(None)

        selector.register(sock.fileno(),
                          EVENT_WRITE,
                          on_connected)
        yield f
        selector.unregister(sock.fileno())
        print('connected!')
```

Now fetch is a generator function, rather than a regular one, because it contains a yield statement. We create a pending future, 
then yield it to pause fetch until the socket is ready. The inner function on_connected resolves the future.

But when the future resolves, what resumes the generator? We need a coroutine driver. Let us call it "task":

```
class Task:
    def __init__(self, coro):
        self.coro = coro
        f = Future()
        f.set_result(None)
        self.step(f)

    def step(self, future):
        try:
            next_future = self.coro.send(future.result)
        except StopIteration:
            return

        next_future.add_done_callback(self.step)

# Begin fetching http://xkcd.com/353/
fetcher = Fetcher('/353/')
Task(fetcher.fetch())

loop()
```

The task starts the ```fetch``` generator by sending ```None``` into it. Then ```fetch``` runs until it yields a future, which the task captures as ```next_future```. 
When the socket is connected, the event loop runs the callback ```on_connected```, which resolves the future, which calls ```step```, which resumes ```fetch```.


## Factoring Coroutines With yield from 使用```yield from```构造协同程序

## 协调协同程序

## 结论

***
## 注
1. 龟叔在[PyCon 2013](http://pyvideo.org/video/1667/keynote)上介绍了标准asyncio库，当时叫做“Tulip”。
2. Even calls to ```send``` can block, if the recipient is slow to acknowledge outstanding messages and the system's buffer of outgoing data is full.
call to send 可以分块，如果接收方迟迟难以确认信号，而且系统传出数据缓冲耗尽。
3. [http://www.kegel.com/c10k.html](http://www.kegel.com/c10k.html)
4. Python's global interpreter lock prohibits running Python code in parallel in one process anyway.
Parallelizing CPU-bound algorithms in Python requires multiple processes, or writing the parallel portions of the code in C. But that is a topic for another day.

5. Jesse listed indications and contraindications for using async in ["What Is Async, How Does It Work, And When Should I Use It?"](http://pyvideo.org/video/2565/what-is-async-how-does-it-work-and-when-should):.
 Mike Bayer compared the throughput of asyncio and multithreading for different workloads in ["Asynchronous Python and Databases"](http://techspot.zzzeek.org/2015/02/15/asynchronous-python-and-databases/):
 
6. For a complex solution to this problem, see http://www.tornadoweb.org/en/stable/stack_context.html

7. The ```@asyncio.coroutine``` decorator is not magical. In fact, if it decorates a generator function and the ```PYTHONASYNCIODEBUG``` environment variable is not set, 
the decorator does practically nothing. It just sets an attribute, ```_is_coroutine```, for the convenience of other parts of the framework. 
It is possible to use asyncio with bare generators not decorated with ```@asyncio.coroutine``` at all.

8. Python 3.5's built-in coroutines are described in [PEP 492](https://www.python.org/dev/peps/pep-0492/), "Coroutines with async and await syntax."

9. This future has many deficiencies. For example, once this future is resolved, a coroutine that yields it should resume immediately instead of pausing, 
but with our code it does not. See asyncio's Future class for a complete implementation.

10. In fact, this is exactly how "yield from" works in CPython. A function increments its instruction pointer before executing each statement. 
But after the outer generator executes "yield from", it subtracts 1 from its instruction pointer to keep itself pinned at the "yield from" statement. 
Then it yields to its caller. The cycle repeats until the inner generator throws ```StopIteration```, 
at which point the outer generator finally allows itself to advance to the next instruction.

11. https://docs.python.org/3/library/queue.html
12. https://docs.python.org/3/library/asyncio-sync.html
13. The actual ```asyncio.Queue``` implementation uses an ```asyncio.Event``` in place of the Future shown here. The difference is an Event can be reset, 
whereas a Future cannot transition from resolved back to pending.

14. https://glyph.twistedmatrix.com/2014/02/unyielding.html