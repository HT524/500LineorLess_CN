# A Web Crawler With asyncio Coroutines

这是[开源程序架构](http://aosabook.org/en/index.html)系列的第四本[500 Lines or Less](https://github.com/aosabook/500lines/blob/master/README.md)的早期章节。
如果你发现任何问题，可以在我们的[Github追踪器](https://github.com/aosabook/500lines/issues)上反馈。
请关注[AOSA blog](http://aosabook.org/blog/)或新的章节和最后的出版计划，新闻公告[推特](https://twitter.com/aosabook), 获取关于本书的最新消息。
***

A. Jesse Jiryu Davis is a staff engineer at MongoDB in New York. He wrote Motor, the async MongoDB Python driver,
 and he is the lead developer of the MongoDB C Driver and a member of the PyMongo team. He contributes to asyncio and Tornado.
 He writes at http://emptysqua.re.  
A. Jesse Jiryu Davis在纽约为MongoDB工作。他编写了Motor，异步MongoDB Python驱动器，他也是MongoDB C驱动器的首席开发者，
同时他也是PyMango组织的成员之一。他对asyncio和Tornado同样有着杰出贡献。他的博客是 http://emptysqua.re .

Guido van Rossum is the creator of Python, one of the major programming languages on and off the web.
The Python community refers to him as the BDFL (Benevolent Dictator For Life), a title straight from a Monty Python skit.
Guido's home on the web is http://www.python.org/~guido/.  
Guido van Rossum，Python之父，Python是目前主要的编程语言之一，无论线上线下。
他在社区里一直是一位仁慈的独裁者，一个来自Monty Python短剧的标题。Guido网上的家是http://www.python.org/~guido/ .


## 介绍

Classical computer science emphasizes efficient algorithms that complete computations as quickly as possible.
But many networked programs spend their time not computing, but holding open many connections that are slow,
or have infrequent events. These programs present a very different challenge: to wait for a huge number of network events efficiently.
A contemporary approach to this problem is asynchronous I/O, or "async".  
经典计算机科学看重高效的算法以便能尽快完成计算。但是许多网络程序消耗的时间不是在计算上，它们通常维持着许多打开的缓慢的连接，或者期待着一些不频繁发生的事件发生。这些程序代表了另一个不同的挑战：如何高效的监听大量网络事件。解决这个问题的一个现代方法是采用异步I/O.

This chapter presents a simple web crawler. The crawler is an archetypal async application because it waits for many responses,
but does little computation. The more pages it can fetch at once, the sooner it completes.
If it devotes a thread to each in-flight request, then as the number of concurrent requests rises it will run out of memory or
other thread-related resource before it runs out of sockets. It avoids the need for threads by using asynchronous I/O.  
这一章节实现了一个简单的网络爬虫。这个爬虫是一个异步调用的原型应用程序，因为它需要等待许多响应，而极少有CPU计算。它每次可以抓取的页面越多，它运行结束的时间越快。
如果它为每一个运行的请求分发一个线程，那么随着并发请求数量的增加，它最终会在耗尽系统套接字之前，耗尽内存或者其他线程相关的资源。
它通过使用异步I/O来避免对大量线程依赖。

We present the example in three stages. First, we show an async event loop and sketch a crawler that uses the event loop
with callbacks: it is very efficient, but extending it to more complex problems would lead to unmanageable spaghetti code.
Second, therefore, we show that Python coroutines are both efficient and extensible.
We implement simple coroutines in Python using generator functions. In the third stage,
we use the full-featured coroutines from Python's standard "asyncio" library1, and coordinate them using an async queue.  
我们通过三步来实现这个例子。首先，我们展示一个异步事件循环并且梗概一个通过回掉使用这个事件循环。它非常高效，但是扩展它去适应更复杂的问题时会
导致难以处理的意大利面条式代码。因而接下来我们展示既高效又易扩展的Python协同程序。我们在Python中使用生成器函数来实现简单的协调程序。
最后，我们使用来自Python标准“asyncio”库中的全功能的协程程序，然后使用异步序列来整合他们。


## 任务
A web crawler finds and downloads all pages on a website, perhaps to archive or index them. Beginning with a root URL,
it fetches each page, parses it for links to pages it has not seen, and adds the new links to a queue.
When it fetches a page with no unseen links and the queue is empty, it stops.  
一个网络爬虫寻找并下载一个网站上的所有页面，可能会存储，或者对它们建立索引。由一个根地址开始，取得每一个页面，解析它去寻找指向从未访问过的页面的链接，把新的链接加入队列。
当他解析到一个没有包含陌生链接的页面并且队列是空的，它便停下来。

We can hasten this process by downloading many pages concurrently. As the crawler finds new links,
it launches simultaneous fetch operations for the new pages on separate sockets. It parses responses as they arrive, adding new links to the queue.
There may come some point of diminishing returns where too much concurrency degrades performance, so we cap the number of concurrent requests,
and leave the remaining links in the queue until some in-flight requests complete.  
我们可以通过同时下载许多页面来加快这个过程。当爬虫发现新的链接时，它在单独的套接字上同时启动抓取新页面的操作。当抓取结果抵达时，它开始解析响应，并往队列里添加新解析到的链接。
大量的并发请求可能导致一些性能降低，因而我们限制同一时间内请求的数量，把其他的链接加入队列直到一些运行中的请求完成。


## 传统的实现方法
How do we make the crawler concurrent? Traditionally we would create a thread pool.
Each thread would be in charge of downloading one page at a time over a socket. For example, to download a page from xkcd.com。  
我们该如何让爬虫并发处理请求呢？传统方法是建立一个线程池。每个进程每次将负责通过一个套接字下载一个页面。比如，下载“xkcd.com”的一个页面。  

```
def fetch(url):
    sock = socket.socket()
    sock.connect(('xkcd.com', 80))
    request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
    sock.send(request.encode('ascii'))
    response = b''
    chunk = sock.recv(4096)
    while chunk:
        response += chunk
        chunk = sock.recv(4096)

    # Page is now downloaded.
    links = parse_links(response)
    q.add(links)
```

By default, socket operations are blocking: when the thread calls a method like connect or recv, it pauses until the operation completes.
2 Consequently to download many pages at once, we need many threads. A sophisticated application amortizes the cost of thread-creation
by keeping idle threads in a thread pool, then checking them out to reuse them for subsequent tasks; it does the same with sockets in a connection pool.  
默认情况下，套接字操作是阻塞的：当一个线程调用一个像```connect```或者```recv```之类Socket相关的方法时，它会被阻塞直至操作完成。
另外，一次性并行下载很多页面，我们得需要更多的线程。一个复杂点的程序，会将线程频繁创建的开销通过在线程池中保存空闲线程的方式摊销，然后再从线程池中取出并重用这些线程去处理随后的任务；这样达到的效果和使用Socket连接池一样


And yet, threads are expensive, and operating systems enforce a variety of hard caps on the number of threads a process, user, or machine may have.
On Jesse's system, a Python thread costs around 50k of memory, and starting tens of thousands of threads causes failures.
If we scale up to tens of thousands of simultaneous operations on concurrent sockets, we run out of threads before we run out of sockets.
Per-thread overhead or system limits on threads are the bottleneck.  
然而，线程的开销是相对昂贵的，操作系统执行```TODO```
在Jesse的电脑上，一个Python线程大约消耗50K内存，并且开启成千上万个线程的时候会失败。
如果我们使用并发Socket的方式同时采取成千上万的操作，我们会在耗尽Socket之前达到我们能使用的线程的上限。每一个的开销或者操作系统的上限是这种实现方式的瓶颈。


In his influential article "The C10K problem"3, Dan Kegel outlines the limitations of multithreading for I/O concurrency. He begins,  
在他那篇颇有影响力的文章《The C10K problem》中，Dan Kegel概述了用多线程并行处理I/O问题的局限性。

> It's time for web servers to handle ten thousand clients simultaneously, don't you think? After all, the web is a big place now.
> 是时候让web服务器同时处理数万客户端请求了，不是吗？毕竟，web那么大。

Kegel coined the term "C10K" in 1999. Ten thousand connections sounds dainty now, but the problem has changed only in size,
not in kind. Back then, using a thread per connection for C10K was impractical. Now the cap is orders of magnitude higher.
Indeed, our toy web crawler would work just fine with threads. Yet for very large scale applications,
with hundreds of thousands of connections, the cap remains: there is a limit beyond which most systems can still create sockets,
but have run out of threads. How can we overcome this?  
Kegel在1999年发明了“C10K”这个词。一万连接现在听起来觉得很少，但问题的关键点在于连接的数量而不在于类型。回到那个年代，一个连接使用一个线程来处理C10K问题是不实际的。现在容量已经是当初的好几个数量级了。说实话，我们的爬虫小玩具使用线程的方式也能运行的很好。但对于需要面对成百上千连接的大规模应用程序来说，使用线程的缺陷还是依旧在这儿：大部分操作系统还能创建Socket，但是不能再继续创建线程了。我们如何克服这个难题呢？


## 异步
Asynchronous I/O frameworks do concurrent operations on a single thread using non-blocking sockets. In our async crawler, we set the socket non-blocking before we begin to connect to the server:  
异步I/O框架通过在一个线程内使用非阻塞socket实现并发操作。在我们的异步小爬虫里，我们会在连接Server前设置socket工作在非阻塞模式。  
```
sock = socket.socket()
sock.setblocking(False)
try:
    sock.connect(('xkcd.com', 80))
except BlockingIOError:
    pass
```

Irritatingly, a non-blocking socket throws an exception from ```connect```, even when it is working normally. This exception replicates the irritating behavior of the underlying C function, which sets ```errno``` to ```EINPROGRESS``` to tell you it has begun.  
比较烦人的是，一个非阻塞的socket会从```connect```方法中抛出一个异常，即使它在正常工作的时候。这个异常其实是从底层对应的C函数复制过来的，在连接正在建立的时候这个函数会返回并将一个全局变量```errno```的值设置为```EINPROGRESS```。  

Now our crawler needs a way to know when the connection is established, so it can send the HTTP request. We could simply keep trying in a tight loop:  
现在我们的小爬虫需要一个方式来知道连接已经建立了，然后它可以发送HTTP请求给Server。我们可以简单的把这个操作放在一个循环里：
```
request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
encoded = request.encode('ascii')

while True:
    try:
        sock.send(encoded)
        break  # Done.
    except OSError as e:
        pass

print('sent')
```

This method not only wastes electricity, but it cannot efficiently await events on multiple sockets. In ancient times, BSD Unix's solution to this problem was ```select```, a C function that waits for an event to occur on a non-blocking socket or a small array of them. Nowadays the demand for Internet applications with huge numbers of connections has led to replacements like ```poll```, then ```kqueue``` on BSD and ```epoll``` on Linux. These APIs are similar to ```select```, but perform well with very large numbers of connections.  
这个实现方法不仅浪费电力，而且不能在多个socket上同时等待想要的事件到来。在很久以前，BSD Unix对这个问题的解决办法是使用```select```函数，它会在一个非阻塞socket或者一个非阻塞socket上，等待一个事情的到来。时至今日，为了应对大规模连接，网络应用程序已经使用了类似```poll```的函数来替代它，这个函数在BSD上的实现叫做```kqueue```，在Linux上叫做```epoll```。这些API的功能和```select```很相似，但是性能却非常好，能够支撑起数量非常大的连接。  

Python 3.4's DefaultSelector uses the best select-like function available on your system. To register for notifications about network I/O, we create a non-blocking socket and register it with the default selector:  
Python 3.4中的DefaultSelector会在你的操作系统中自动使用select类似的函数。为了收到网络I/O相关的通知，我们需要注册，通过创建一个非阻塞的socket并且向DefaultSelector来进行注册来实现：
```
from selectors import DefaultSelector, EVENT_WRITE

selector = DefaultSelector()

sock = socket.socket()
sock.setblocking(False)
try:
    sock.connect(('xkcd.com', 80))
except BlockingIOError:
    pass

def connected():
    selector.unregister(sock.fileno())
    print('connected!')

selector.register(sock.fileno(), EVENT_WRITE, connected)
```

We disregard the spurious error and call ```selector.register```, passing in the socket's file descriptor and a constant that expresses what event we are waiting for. To be notified when the connection is established, we pass ```EVENT_WRITE```: that is, we want to know when the socket is "writable". We also pass a Python function, ```connected```, to run when that event occurs. Such a function is known as a callback.  
我们忽略EINPROGRESS这个伪错误，然后调用```selector.register```方法，前两个参数里，我们传递给它打开的socket文件描述符，和一个代表了我们所想等待的事件的常量。为了能在连接建立好时我们可以收到通知，我们传递给它```EVENT_WRITE```这个常量，这个常量的含义是告诉系统我们想在这个socket可写的时候能及时知道。同时，我们也传递给它一个Python函数```connected```，这个函数会在事件发生的时候运行。这样的一种函数被叫做“回调函数”。  

We process I/O notifications as the selector receives them, in a loop:  
当selector接收到了事件的时候，我们在一个循环中处理I/O通知：
```
def loop():
    while True:
        events = selector.select()
        for event_key, event_mask in events:
            callback = event_key.data
            callback()
```

The ```connected``` callback is stored as event_key.data, which we retrieve and execute once the non-blocking socket is connected.  
回调函数```connected```被存储在event_key.data中，一旦这个非阻塞socket连接了，我们会取出这个回调函数并执行它。  

Unlike in our fast-spinning loop above, the call to ```select``` here pauses, awaiting the next I/O events. Then the loop runs callbacks that are waiting for these events. Operations that have not completed remain pending until some future tick of the event loop.  
不像我们之前那个快速的循环，调用```select```会暂停代码的往下执行，等待下一个I/O事件的到来才继续往下执行。事件到来之后，这个循环会继续执行，调用等待那些事件出现的回调函数。在一次事件中未完成的操作会处于等待状态直到未来下一次的事件循环的发生。  

We have achieved "concurrency" here, but not what is traditionally called "parallelism". That is, we built a tiny system that does overlapping I/O. It is capable of beginning new operations while others are in flight. It does not actually utilize multiple cores to execute computation in parallel. But then, this system is designed for I/O-bound problems, not CPU-bound ones.  
这里我们其实已经实现了“并发”，但这不是传统意义上的并行。也就是说，我们建立了一个可以同时执行I/O操作的微型系统。它能够在别人还在等待中的之后就开始新的操作。它也并不是真正利用了多核来并行地执行计算。因为，这个系统是针对I/O密集型设计的，不是CPU密集型。  

So our event loop is efficient at concurrent I/O because it does not devote thread resources to each connection. But before we proceed, it is important to correct a common misapprehension that async is faster than multithreading. Often it is not—indeed, in Python, an event loop like ours is moderately slower than multithreading at serving a small number of very active connections. In a runtime without a global interpreter lock, threads would perform even better on such a workload. What asynchronous I/O is right for, is applications with many slow or sleepy connections with infrequent events.  
所以我们的事件循环在并发I/O情景下是高效的，因为它不会为每个连接分配一个线程的资源。但是在我们继续往下之前，要纠正一个很重要的误解，那就是异步会比多线程快。通常情况下它并不快，在Python里，像我们前面所写的利用循环等待事件的代码，稍微比多线程的版本慢。在一个没有全局解释器锁的运行时环境下，多线的性能表现会更好。异步I/O所使用的场景，是设计并实现那些需要处理大量空闲慢连接的应用。


## 回调
With the runty async framework we have built so far, how can we build a web crawler? Even a simple URL-fetcher is painful to write.  
我们前面搭建了一个小小的异步框架，但是它对于我们构建一个网页爬虫有什么帮助呢？因为甚至一个简单的URL抓取器都是很难写（请看下文）。
我们从简单的存储抓取到的URL的全局变量开始，这两个URL我们前面见过：
```
urls_todo = set(['/'])
seen_urls = set(['/'])
```

The ```seen_urls``` set includes urls_todo plus completed URLs. The two sets are initialized with the root URL "/".  
```seen_urls```这个set变量里的值包含```url_todo```变量的值再加上一句完成抓取的URL。这两个set初始化为根URL "/"  

Fetching a page will require a series of callbacks. The ```connected``` callback fires when a socket is connected, and sends a GET request to the server. But then it must await a response, so it registers another callback. If, when that callback fires, it cannot read the full response yet, it registers again, and so on.  
抓取一个页面需要一系列的回调函数。```connected```回调函数在当一个socket连接准备好的时候进行触发，然后它像server发送请求。但它必须等到一个响应，所以它又必须注册另一个回调函数。如果，当回调函数触发的时候它还仍不能读取完整的响应数据，它会再次注册这个函数，如此反复。  

Let us collect these callbacks into a Fetcher object. It needs a URL, a socket object, and a place to accumulate the response bytes:  
让我们把这些回调函数都封装在一个叫做```Fetcher```的对象里吧，它需要一个URL，一个socket对象还有一个可以累计响应数据的地方：
```
class Fetcher:
    def __init__(self, url):
        self.response = b''  # Empty array of bytes.
        self.url = url
        self.sock = None
```

We begin by calling ```Fetcher.fetch```:  
我们从调用```Fetcher.fetch```开始：
```
    # Method on Fetcher class.
    def fetch(self):
        self.sock = socket.socket()
        self.sock.setblocking(False)
        try:
            self.sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass

        # Register next callback.
        selector.register(self.sock.fileno(),
                          EVENT_WRITE,
                          self.connected)
```

The ```fetch``` method begins connecting a socket. But notice the method returns before the connection is established. It must return control to the event loop to wait for the connection. To understand why, imagine our whole application was structured so:  
```fetch```方法开始于通过一个socket连接server。但是请注意，这个方法会在连接建立好之前就会返回。它必须把控制权返回交给接下来的事件循环中，那里会等在着连接完成。为了明白为什么这样做，想象一下我们的整个应用是这样子构建的：
```
# Begin fetching http://xkcd.com/353/
fetcher = Fetcher('/353/')
fetcher.fetch()

while True:
    events = selector.select()
    for event_key, event_mask in events:
        callback = event_key.data
        callback(event_key, event_mask)
```

All event notifications are processed in the event loop when it calls ```select```. Hence ```fetch``` must hand control to the event loop, so that the program knows when the socket has connected. Only then does the loop run the ```connected``` callback, which was registered at the end of ```fetch``` above.  
当```select```函数被调用并返回之后，所有事件通知都会在事件循环中得到处理。因此```fetch```必须函数必须把控制权交给接下来的事件循环，好让程序在socket连接的时候能及时知道。直到那个时候，这个循环才会去调用```connected```回调函数，这个回调函数是在上面的```fetch```函数尾部注册的。

Here is the implementation of ```connected```:  
下面是```connected```函数的实现：
```
    # Method on Fetcher class.
    def connected(self, key, mask):
        print('connected!')
        selector.unregister(key.fd)
        request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(self.url)
        self.sock.send(request.encode('ascii'))

        # Register the next callback.
        selector.register(key.fd,
                          EVENT_READ,
                          self.read_response)
```

The method sends a GET request. A real application would check the return value of ```send``` in case the whole message cannot be sent at once. But our request is small and our application unsophisticated. It blithely calls ```send```, then waits for a response. Of course, it must register yet another callback and relinquish control to the event loop. The next and final callback, ```read_response```, processes the server's reply:  
```connected```方法发送一个GET请求，一个真正的应用其实会检查```send```函数的返回值以防整个消息没有被一次性完全发送完。但我们是请求很小而且也不复杂，它只是简单地调用```send```函数然后直接等待响应就行了。当然了，它必须注册另一个回调函数，然后放弃控制权并将其交还回事件循环。下面一个也是最后一个回调函数```read_response```会处理服务器的应答：
```
    # Method on Fetcher class.
    def read_response(self, key, mask):
        global stopped

        chunk = self.sock.recv(4096)  # 4k chunk size.
        if chunk:
            self.response += chunk
        else:
            selector.unregister(key.fd)  # Done reading.
            links = self.parse_links()

            # Python set-logic:
            for link in links.difference(seen_urls):
                urls_todo.add(link)
                Fetcher(link).fetch()  # <- New Fetcher.

            seen_urls.update(links)
            urls_todo.remove(self.url)
            if not urls_todo:
                stopped = True
```

The callback is executed each time the selector sees that the socket is "readable", which could mean two things: the socket has data or it is closed.  
这个回调函数会在selector每次发现socket都"可读"的时候被调用，这意味着两件事情：socket里有数据可读或者它被关闭了。  

The callback asks for up to four kilobytes of data from the socket. If less is ready, ```chunk``` contains whatever data is available. If there is more, ```chunk``` is four kilobytes long and the socket remains readable, so the event loop runs this callback again on the next tick. When the response is complete, the server has closed the socket and ```chunk``` is empty.  
这个回调函数会向socket请求一次读取4k字节的数据。如果还没有那么多数据准备好，```chunk```只会存储着准备好的数据。如果有超过4k字节的数据可读，那么```chunk```也是最多能读取并保存4k字节长的数据，并且socket还是“可读的”，所以事件循环会在下一轮继续调用这个此回调函数。当响应完成了，服务器会关闭掉socekt，此时```chunk```就会是空的了。  

The ```parse_links``` method, not shown, returns a set of URLs. We start a new fetcher for each new URL, with no concurrency cap. Note a nice feature of async programming with callbacks: we need no mutex around changes to shared data, such as when we add links to ```seen_urls```. There is no preemptive multitasking, so we cannot be interrupted at arbitrary points in our code.  
```parse_links```方法这里没有展示，它会返回一个URL的集合。我们为每一个新的URL开启一个没有协程支持的fetcher。注意，使用回调进行异步编程的好处：我们不需要锁来应对共享数据的变更，例如我们给```seen_urls```集合添加新链接。这里没有实际的多任务，我们的代码在执行过程中不会在任何地方被打断。   


We add a global ```stopped``` variable and use it to control the loop:  
我们添加一个全局变量```stopped```来控制循环：  

```
stopped = False

def loop():
        while not stopped:
                events = selector.select()
                for event_key, event_mask in events:
                    callback = event_key.data
                    callback()

```

Once all pages are downloaded the fetcher stops the global event loop and the program exits.  
一旦所有的页面都下载完毕，fetcher就会停止这个全局的事件循环，然后程序退出。  

This example makes async's problem plain: spaghetti code. We need some way to express a series of computations and I/O operations, and schedule multiple such series of operations to run concurrently. But without threads, a series of operations cannot be collected into a single function: whenever a function begins an I/O operation, it explicitly saves whatever state will be needed in the future, then returns. You are responsible for thinking about and writing this state-saving code.  
这个例子很好的揭露了异步编程的问题：“面条式”的代码。我们需要一些手段来表达计算和I/O操作，同步地调度这一系列的操作。但是没有线程的支持，一系列的操作不能被放到单独的一个函数里：无论何时开启一个I/O操作时，它都会显示地保存将来可能需要的所有状态，然后再返回。  

Let us explain what we mean by that. Consider how simply we fetched a URL on a thread with a conventional blocking socket:  
让我们来解释下刚才说了啥。考虑下面的代码，简单地在一个线程中使用传统的阻塞I/O方式抓取URL：  
```
# Blocking version.
def fetch(url):
        sock = socket.socket()
            sock.connect(('xkcd.com', 80))
            request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
                sock.send(request.encode('ascii'))
                    response = b''
                        chunk = sock.recv(4096)
        while chunk:
                response += chunk
                        chunk = sock.recv(4096)

        # Page is now downloaded.
        links = parse_links(response)
        q.add(links)
```

What state does this function remember between one socket operation and the next? It has the socket, a URL, and the accumulating response. A function that runs on a thread uses basic features of the programming language to store this temporary state in local variables, on its stack. The function also has a "continuation"—that is, the code it plans to execute after I/O completes. The runtime remembers the continuation by storing the thread's instruction pointer. You need not think about restoring these local variables and the continuation after I/O. It is built in to the language.  
在一个socket操作和下一个socket可以操作的期间，这个函数应该记住什么状态呢？它有一个socket，一个URL，然后等待着积攒着响应结果。一个运行在线程里，使用着一门编程语言基本特征的函数会把临时变量存储在它的栈上。这个函数还有“后续”——也即是在I/O操作完成之后它打算运行的代码。运行时通过记录线程的指令指针来保存程序的“连续性”。你不需要考虑如歌恢复这些局部变量，从而能在I/O操作之后继续执行代码。这是语言內建的。  

But with a callback-based async framework, these language features are no help. While waiting for I/O, a function must save its state explicitly, because the function returns and loses its stack frame before I/O completes. In lieu of local variables, our callback-based example stores sock and response as attributes of self, the Fetcher instance. In lieu of the instruction pointer, it stores its continuation by registering the callbacks connected and read_response. As the application's features grow, so does the complexity of the state we manually save across callbacks. Such onerous bookkeeping makes the coder prone to migraines.  
但如果有基于回调的异步框架支持的话，那些语言特性就没有帮助了。当在等待I/O操作结束时，一个函数一定要显示地保存它的所有状态，因为函数会在I/O结束前失去它的栈帧。在替代的局部变量里，我们基于回调的例子，TODO（这里真的不懂怎么翻译）  


Even worse, what happens if a callback throws an exception, before it schedules the next callback in the chain? Say we did a poor job on the ```parse_links``` method and it throws an exception parsing some HTML:
更糟糕的，当一个回调函数抛出议程的时候会发生上面，特别是在调度调用链上下一个回调之前？假设我们利用```parse_links```方法做了一个很蠢的任务，然后它在解析HTML的时候抛出了一个异常：  

```
Traceback (most recent call last):
      File "loop-with-callbacks.py", line 111, in <module>
          loop()
        File "loop-with-callbacks.py", line 106, in loop
          callback(event_key, event_mask)
        File "loop-with-callbacks.py", line 51, in read_response
          links = self.parse_links()
        File "loop-with-callbacks.py", line 67, in parse_links
          raise Exception('parse error')
      Exception: parse error
```

The stack trace shows only that the event loop was running a callback. We do not remember what led to the error. The chain is broken on both ends: we forgot where we were going and whence we came. This loss of context is called "stack ripping", and in many cases it confounds the investigator. Stack ripping also prevents us from installing an exception handler for a chain of callbacks, the way a "try / except" block wraps a function call and its tree of descendents.  
这个栈的跟踪显示了事件循环当时正在运行着。我们不没看到哪里记录了这个错误的发生。调用链在两端都已经被破换：我们忘记了打算到到哪里去，也忘记了我们从哪里来。这样的上下文丢失叫做“栈破坏”，并且它在很多时候使得依赖跟踪堆栈信息的人感到困惑。栈破坏也会阻止我们设置调用链的异常回调处理函数，就是"try / except" 包裹着被调用函数及其调用的函数这种方式。  

So, even apart from the long debate about the relative efficiencies of multithreading and async, there is this other debate regarding which is more error-prone: threads are susceptible to data races if you make a mistake synchronizing them, but callbacks are stubborn to debug due to stack ripping.  
所以，抛开关于多线程异步效率的问题的漫长讨论，还有其他关于哪个更易于出错的讨论：线程对数据竞争跟敏感些，如果你忘记对共享数据进行同步处理的话，但是回调的话由于栈破坏的存在使得难于调试。  



## 协程


## Python生成器如何工作


## 使用生成器实现协程


So a generator can pause, and it can be resumed with a value, and it has a return value. Sounds like a good primitive upon which to build an async programming model,
without spaghetti callbacks! We want to build a "coroutine": a routine that is cooperatively scheduled with other routines in the program.
Our coroutines will be a simplified version of those in Python's standard "asyncio" library. As in asyncio, we will use generators, futures,
and the "yield from" statement.

First we need a way to represent some future result that a coroutine is waiting for. A stripped-down version:

```
class Future:
    def __init__(self):
        self.result = None
        self._callbacks = []

    def add_done_callback(self, fn):
        self._callbacks.append(fn)

    def set_result(self, result):
        self.result = result
        for fn in self._callbacks:
            fn(self)
```

A future is initially "pending". It is "resolved" by a call to ```set_result```.9

Let us adapt our fetcher to use futures and coroutines. Review how we wrote ```fetch``` with a callback:

```
class Fetcher:
    def fetch(self):
        self.sock = socket.socket()
        self.sock.setblocking(False)
        try:
            self.sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass
        selector.register(self.sock.fileno(),
                          EVENT_WRITE,
                          self.connected)

    def connected(self, key, mask):
        print('connected!')
        # And so on....
```

The fetch method begins connecting a socket, then registers the callback, connected, to be executed when the socket is ready.
Now we can combine these two steps into one coroutine:

```
    def fetch(self):
        sock = socket.socket()
        sock.setblocking(False)
        try:
            sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass

        f = Future()

        def on_connected():
            f.set_result(None)

        selector.register(sock.fileno(),
                          EVENT_WRITE,
                          on_connected)
        yield f
        selector.unregister(sock.fileno())
        print('connected!')
```

Now fetch is a generator function, rather than a regular one, because it contains a yield statement. We create a pending future,
then yield it to pause fetch until the socket is ready. The inner function on_connected resolves the future.

But when the future resolves, what resumes the generator? We need a coroutine driver. Let us call it "task":

```
class Task:
    def __init__(self, coro):
        self.coro = coro
        f = Future()
        f.set_result(None)
        self.step(f)

    def step(self, future):
        try:
            next_future = self.coro.send(future.result)
        except StopIteration:
            return

        next_future.add_done_callback(self.step)

# Begin fetching http://xkcd.com/353/
fetcher = Fetcher('/353/')
Task(fetcher.fetch())

loop()
```

The task starts the ```fetch``` generator by sending ```None``` into it. Then ```fetch``` runs until it yields a future, which the task captures as ```next_future```.
When the socket is connected, the event loop runs the callback ```on_connected```, which resolves the future, which calls ```step```, which resumes ```fetch```.


## Factoring Coroutines With yield from 使用```yield from```构造协同程序

## 协调协同程序

## 结论

***
## 注
1. 龟叔在[PyCon 2013](http://pyvideo.org/video/1667/keynote)上介绍了标准asyncio库，当时叫做“Tulip”。
2. Even calls to ```send``` can block, if the recipient is slow to acknowledge outstanding messages and the system's buffer of outgoing data is full.
call to send 可以分块，如果接收方迟迟难以确认信号，而且系统传出数据缓冲耗尽。
3. [http://www.kegel.com/c10k.html](http://www.kegel.com/c10k.html)
4. Python's global interpreter lock prohibits running Python code in parallel in one process anyway.
Parallelizing CPU-bound algorithms in Python requires multiple processes, or writing the parallel portions of the code in C. But that is a topic for another day.

5. Jesse listed indications and contraindications for using async in ["What Is Async, How Does It Work, And When Should I Use It?"](http://pyvideo.org/video/2565/what-is-async-how-does-it-work-and-when-should):.
 Mike Bayer compared the throughput of asyncio and multithreading for different workloads in ["Asynchronous Python and Databases"](http://techspot.zzzeek.org/2015/02/15/asynchronous-python-and-databases/):

6. For a complex solution to this problem, see http://www.tornadoweb.org/en/stable/stack_context.html

7. The ```@asyncio.coroutine``` decorator is not magical. In fact, if it decorates a generator function and the ```PYTHONASYNCIODEBUG``` environment variable is not set,
the decorator does practically nothing. It just sets an attribute, ```_is_coroutine```, for the convenience of other parts of the framework.
It is possible to use asyncio with bare generators not decorated with ```@asyncio.coroutine``` at all.

8. Python 3.5's built-in coroutines are described in [PEP 492](https://www.python.org/dev/peps/pep-0492/), "Coroutines with async and await syntax."

9. This future has many deficiencies. For example, once this future is resolved, a coroutine that yields it should resume immediately instead of pausing,
but with our code it does not. See asyncio's Future class for a complete implementation.

10. In fact, this is exactly how "yield from" works in CPython. A function increments its instruction pointer before executing each statement.
But after the outer generator executes "yield from", it subtracts 1 from its instruction pointer to keep itself pinned at the "yield from" statement.
Then it yields to its caller. The cycle repeats until the inner generator throws ```StopIteration```,
at which point the outer generator finally allows itself to advance to the next instruction.

11. https://docs.python.org/3/library/queue.html
12. https://docs.python.org/3/library/asyncio-sync.html
13. The actual ```asyncio.Queue``` implementation uses an ```asyncio.Event``` in place of the Future shown here. The difference is an Event can be reset,
whereas a Future cannot transition from resolved back to pending.

14. https://glyph.twistedmatrix.com/2014/02/unyielding.html
